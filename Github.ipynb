{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMSE Project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I load in the data and spit out some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014 crime: (3177, 56) // key: (41787, 7)\n",
      "2019 Housing: (1921, 17)\n",
      "2018 Wages: (62724, 21)\n",
      "7313843 unique datapoints in all datasets\n"
     ]
    }
   ],
   "source": [
    "# Reading in Data\n",
    "# Keys to extract formatted State and County names\n",
    "key = pd.read_excel('fips_codes_website.xls')   # Counties and major cities\n",
    "key00 = pd.read_csv('FIPS_countycodes.txt',delimiter=',')  # Counties only\n",
    "key00['GU Name'] = key00['GU Name'].str[:-7]  # Slice 'county' from names\n",
    "# Housing Data\n",
    "zillow2019 = pd.read_excel(\"County_Zhvi_Summary_AllHomes2019.xls\")\n",
    "zillow2019['RegionName'] = zillow2019['RegionName'].str[:-7]  # Slice 'county' from names\n",
    "# Crime\n",
    "crime2010 = pd.read_csv('33523-0001-crimedata2010.txt',delimiter='\\t')\n",
    "crime2012 = pd.read_csv('35019-0001-crimedata2012.txt',delimiter='\\t')\n",
    "crime2014 = pd.read_csv('36399-0001-crimedata2014.txt',delimiter='\\t')\n",
    "crime2016 = pd.read_csv('37059-0001-crimedata2016.txt',delimiter='\\t')\n",
    "# Enumployment & Wages\n",
    "#test_wages_18 = pd.read_excel('allhlcn183.xlsx',sheet_name='US_St_Cn_MSA')\n",
    "wage_unemp2018 = pd.read_csv('allhlcn183wage_unemp2018.txt',delimiter='\\t',dtype={'St':str})\n",
    "wage_unemp2016 = pd.read_csv('allhlcn163wage_unemp2016.txt',delimiter='\\t',dtype={'St':str})\n",
    "wage_unemp2014 = pd.read_csv('allhlcn143wage_unemp2014.txt',delimiter='\\t',dtype={'St':str})\n",
    "wage_unemp2012 = pd.read_csv('allhlcn123wage_unemp2012.txt',delimiter='\\t',dtype={'St':str})\n",
    "wage_unemp2010 = pd.read_csv('allhlcn103wage_unemp2010.txt',delimiter='\\t',dtype={'St':str})\n",
    "\n",
    "print(\"2014 crime:\",crime2014.shape,\"// key:\",key.shape)\n",
    "print(\"2019 Housing:\",zillow2019.shape)\n",
    "print('2018 Wages:',wage_unemp2018.shape)\n",
    "x = ((crime2014.shape[0])*(crime2014.shape[1]))*4 + ((wage_unemp2018.shape[0])*(wage_unemp2018.shape[1]))*5 + (key00.shape[0])*key00.shape[1]\n",
    "print(x,'unique datapoints in all datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where I do the bulk of my data cleaning. \n",
    "\n",
    "I found it difficult to eliminate the copyset errors because the wage_unemployment dataframes are coded with key values in the County and State columns (as seen above). There is national, state, and metropolitan area data in these dataframes. But the values that I would need to concat/merge on are 'NaN' for these other regional divisions. So I am not able to merge with the dataframe as is, and dropping the problem rows then merging produces the error. \n",
    "\n",
    "The rest of my data cleaning is just converting columns of numbers from strings into integers. The BLS also started a new method of how they code in the various industries within a region, so I also have to clean that up to match past years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def crime_cleaner(data):\n",
    "    # Merge key with crime db to gain state / city names\n",
    "    washing = data.merge(key00,left_on=['FIPS_CTY','FIPS_ST'], right_on=['County FIPS Code','State FIPS Code'])\n",
    "    # Rearrange columns\n",
    "    washing = washing[['STUDYNO','EDITION','PART','IDNO','FIPS_ST','State FIPS Code','State Abbreviation','FIPS_CTY','County FIPS Code','GU Name','CPOPARST','AG_ARRST','JURFLAG','COVIND', 'GRNDTOT','P1TOT','P1VLNT','P1PRPTY','MURDER','RAPE','ROBBERY','AGASSLT','BURGLRY','LARCENY','MVTHEFT','ARSON','OTHASLT','FRGYCNT','FRAUD','EMBEZL','STLNPRP','VANDLSM','WEAPONS','COMVICE','SEXOFF','DRUGTOT','DRGSALE','COCSALE','MJSALE','SYNSALE','OTHSALE','DRGPOSS','COCPOSS','MJPOSS','SYNPOSS','OTHPOSS','GAMBLE','BOOKMKG','NUMBERS','OTGAMBL','OFAGFAM','DUI','LIQUOR','DRUNK','DISORDR','VAGRANT','ALLOTHR','SUSPICN','CURFEW','RUNAWAY']]\n",
    "    \n",
    "    # Select only wanted columns\n",
    "    clean = washing[['State Abbreviation','GU Name','MURDER','P1TOT','P1VLNT','P1PRPTY','RAPE','ROBBERY','AGASSLT','BURGLRY','LARCENY','MVTHEFT','ARSON','OTHASLT','FRGYCNT','FRAUD','EMBEZL','STLNPRP','VANDLSM','WEAPONS','COMVICE','SEXOFF','DRUGTOT','DRGSALE','COCSALE','MJSALE','SYNSALE','OTHSALE','DRGPOSS','COCPOSS','MJPOSS','SYNPOSS','OTHPOSS','GAMBLE','BOOKMKG','NUMBERS','OTGAMBL','OFAGFAM','DUI','LIQUOR','DRUNK','DISORDR','VAGRANT','ALLOTHR','SUSPICN','CURFEW','RUNAWAY']]\n",
    "    return clean\n",
    "def wage_cleaner(data):\n",
    "        # Drop Unwanted columns\n",
    "    base = data.drop(['Area\\nCode','Own','NAICS','Qtr','Status Code'],axis=1)\n",
    "    # Select County level data\n",
    "    washing = base.loc[(base['Area Type'] == 'County')]\n",
    "    washing['Cnty'] = washing['Cnty'].astype('int64') # Convert to int for merging // Throws error\n",
    "    washing['St'] = washing['St'].astype('int64')    # Convert to into for merging // Throws error\n",
    "    # Merge with key\n",
    "    washing = washing.merge(key00,left_on=['St','Cnty'], right_on=['State FIPS Code','County FIPS Code'])\n",
    "    # Rearrange columns and drop unwanted columns\n",
    "    county_wages = washing[['State Abbreviation','GU Name', 'Year', 'Area Type', 'St Name', 'Area', 'Ownership','Industry', 'Establishment Count', 'July Employment','August Employment', 'September Employment', 'Total Quarterly Wages','Average Weekly Wage', 'Employment Location Quotient Relative to U.S.','Total Wage Location Quotient Relative to U.S.']]\n",
    "    county_wages['Establishment Count'] = county_wages['Establishment Count'].str.replace(',','')  # Get rid of comma // Throws Error\n",
    "    county_wages['Establishment Count'] = pd.to_numeric(county_wages['Establishment Count'])   # Convert to interger // Throws Error\n",
    "    county_wages['September Employment'] = county_wages['September Employment'].str.replace(',','')  # Get rid of comma // Throws Error\n",
    "    county_wages['September Employment'] = pd.to_numeric(county_wages['September Employment'])    # Convert to interger // Throws Error  \n",
    "    county_wages['Average Weekly Wage'] = county_wages['Average Weekly Wage'].str.replace(',','')  # Get rid of comma // Throws Error\n",
    "    county_wages['Average Weekly Wage'] = pd.to_numeric(county_wages['Average Weekly Wage'])  # Convert to interger // Throws Error\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('0','') # Formatting to fix year 2018 industry codes\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('1','')\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('2','')\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('3','')\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('4','')\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('5','')\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('6','')\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('7','')\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('8','')\n",
    "    county_wages['Industry'] = county_wages['Industry'].str.replace('9','')\n",
    "    \n",
    "#     state_wages = base.loc[(base['Area Type'] == 'State')]   # State Level Wages\n",
    "#     nation_wages = base.loc[(base['Area Type'] == 'Nation')]    # Nation Level Data\n",
    "#     urban_wages = base.loc[(base['Area Type'] == 'MSA')]   # Metropolitan Level Data\n",
    "    return county_wages #,state_wages,nation_wages,urban_wages\n",
    "\n",
    "def search(county,state):\n",
    "    db10 = db2010.loc[(db2010['State Abbreviation'] == state) & (db2010['GU Name'] == county)]\n",
    "    db12 = db2012.loc[(db2012['State Abbreviation'] == state) & (db2012['GU Name'] == county)]\n",
    "    db14 = db2014.loc[(db2014['State Abbreviation'] == state) & (db2014['GU Name'] == county)]\n",
    "    db16 = db2016.loc[(db2016['State Abbreviation'] == state) & (db2016['GU Name'] == county)]\n",
    "    db18 = economic18.loc[(economic18['State Abbreviation'] == state) & (economic18['GU Name'] == county)]\n",
    "    return db10,db12,db14,db16\n",
    "\n",
    "# Yearly cleaned databases\n",
    "crime10 = crime_cleaner(crime2010)\n",
    "crime12 = crime_cleaner(crime2012)\n",
    "crime14 = crime_cleaner(crime2014)\n",
    "crime16 = crime_cleaner(crime2016)\n",
    "cols = list(crime14.columns.values) # Grab columns\n",
    "# Cleaned Wage Data\n",
    "economic10 = wage_cleaner(wage_unemp2010) # Returns 3 items; county, state, national, and urban level data sets\n",
    "economic12 = wage_cleaner(wage_unemp2012)\n",
    "economic14 = wage_cleaner(wage_unemp2014)\n",
    "economic16 = wage_cleaner(wage_unemp2016)\n",
    "economic18 = wage_cleaner(wage_unemp2018)\n",
    "#wage18[0]['Industry'] = [i[i.find(' ')+1:] for i in wage18[0]['Industry']] # Get to work\n",
    "economic18['Industry'] = economic18['Industry'].str[1:] # Remove first space in 2018 industry codes\n",
    "\n",
    "# Combined crime and economic dataframes and rearrange columns\n",
    "db2010 = crime10.merge(economic10,left_on=['State Abbreviation','GU Name'],right_on=['State Abbreviation','GU Name'])\n",
    "db2010 = db2010[['Year','State Abbreviation','GU Name','MURDER','P1TOT','P1VLNT','P1PRPTY','RAPE','ROBBERY','AGASSLT','BURGLRY','LARCENY','MVTHEFT','ARSON','OTHASLT','FRGYCNT','FRAUD','EMBEZL','STLNPRP','VANDLSM','WEAPONS','COMVICE','SEXOFF','DRUGTOT','DRGSALE','COCSALE','MJSALE','SYNSALE','OTHSALE','DRGPOSS','COCPOSS','MJPOSS','SYNPOSS','OTHPOSS','GAMBLE','BOOKMKG','NUMBERS','OTGAMBL','OFAGFAM','DUI','LIQUOR','DRUNK','DISORDR','VAGRANT','ALLOTHR','SUSPICN','CURFEW','RUNAWAY','Area Type','St Name','Area','Ownership','Industry','Establishment Count','July Employment','August Employment','September Employment','Total Quarterly Wages','Average Weekly Wage','Employment Location Quotient Relative to U.S.','Total Wage Location Quotient Relative to U.S.']]\n",
    "db2012 = crime12.merge(economic12,left_on=['State Abbreviation','GU Name'],right_on=['State Abbreviation','GU Name'])\n",
    "db2012 = db2012[['Year','State Abbreviation','GU Name','MURDER','P1TOT','P1VLNT','P1PRPTY','RAPE','ROBBERY','AGASSLT','BURGLRY','LARCENY','MVTHEFT','ARSON','OTHASLT','FRGYCNT','FRAUD','EMBEZL','STLNPRP','VANDLSM','WEAPONS','COMVICE','SEXOFF','DRUGTOT','DRGSALE','COCSALE','MJSALE','SYNSALE','OTHSALE','DRGPOSS','COCPOSS','MJPOSS','SYNPOSS','OTHPOSS','GAMBLE','BOOKMKG','NUMBERS','OTGAMBL','OFAGFAM','DUI','LIQUOR','DRUNK','DISORDR','VAGRANT','ALLOTHR','SUSPICN','CURFEW','RUNAWAY','Area Type','St Name','Area','Ownership','Industry','Establishment Count','July Employment','August Employment','September Employment','Total Quarterly Wages','Average Weekly Wage','Employment Location Quotient Relative to U.S.','Total Wage Location Quotient Relative to U.S.']]\n",
    "db2014 = crime14.merge(economic14,left_on=['State Abbreviation','GU Name'],right_on=['State Abbreviation','GU Name'])\n",
    "db2014 = db2014[['Year','State Abbreviation','GU Name','MURDER','P1TOT','P1VLNT','P1PRPTY','RAPE','ROBBERY','AGASSLT','BURGLRY','LARCENY','MVTHEFT','ARSON','OTHASLT','FRGYCNT','FRAUD','EMBEZL','STLNPRP','VANDLSM','WEAPONS','COMVICE','SEXOFF','DRUGTOT','DRGSALE','COCSALE','MJSALE','SYNSALE','OTHSALE','DRGPOSS','COCPOSS','MJPOSS','SYNPOSS','OTHPOSS','GAMBLE','BOOKMKG','NUMBERS','OTGAMBL','OFAGFAM','DUI','LIQUOR','DRUNK','DISORDR','VAGRANT','ALLOTHR','SUSPICN','CURFEW','RUNAWAY','Area Type','St Name','Area','Ownership','Industry','Establishment Count','July Employment','August Employment','September Employment','Total Quarterly Wages','Average Weekly Wage','Employment Location Quotient Relative to U.S.','Total Wage Location Quotient Relative to U.S.']]\n",
    "db2016 = crime16.merge(economic16,left_on=['State Abbreviation','GU Name'],right_on=['State Abbreviation','GU Name'])\n",
    "db2016 = db2016[['Year','State Abbreviation','GU Name','MURDER','P1TOT','P1VLNT','P1PRPTY','RAPE','ROBBERY','AGASSLT','BURGLRY','LARCENY','MVTHEFT','ARSON','OTHASLT','FRGYCNT','FRAUD','EMBEZL','STLNPRP','VANDLSM','WEAPONS','COMVICE','SEXOFF','DRUGTOT','DRGSALE','COCSALE','MJSALE','SYNSALE','OTHSALE','DRGPOSS','COCPOSS','MJPOSS','SYNPOSS','OTHPOSS','GAMBLE','BOOKMKG','NUMBERS','OTGAMBL','OFAGFAM','DUI','LIQUOR','DRUNK','DISORDR','VAGRANT','ALLOTHR','SUSPICN','CURFEW','RUNAWAY','Area Type','St Name','Area','Ownership','Industry','Establishment Count','July Employment','August Employment','September Employment','Total Quarterly Wages','Average Weekly Wage','Employment Location Quotient Relative to U.S.','Total Wage Location Quotient Relative to U.S.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code to randomly pull some counties from my database. There is no data attached to them, useful to find random unknown counties to search/test against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Sample of Randomly Selected Counties\n",
    "n = 5 # Numer of random samples\n",
    "print(\"Randomly Generated Cities:\\nState County\")\n",
    "for i in range(n):\n",
    "    r = random.randint(0,len(crime12))\n",
    "    print(crime12['State Abbreviation'].iloc[r]+', '+crime12['GU Name'].iloc[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a search function that will find all available data for any single county. This cell needs to be run for all other cells to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S E A R C H #\n",
    "##############\n",
    "print(\"IF USING 'KEY00', COUNTIES ONLY\")\n",
    "CountCit = input(\"Enter a county: \")\n",
    "state = input(\"Enter a state abbreviation: \")\n",
    "\n",
    "collect = search(CountCit,state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Database of Counties Generation**\n",
    "\n",
    "Here is where I actualy generate a database of random counties and all associated information. The cells immediatly following will automatically plot and display info on whatever single county you searched for. While not specifically useful for analysis I encourage you to browse through them simply to see what variables I have in my data. Otherwise this part can be skipped to where I have a bolded headling about regressions.\n",
    "\n",
    "Each time this cell is ran an entirely new database is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects 10 Random counties and stores them as a dictionary by count // Key = MI, County\n",
    "samp_db = {}\n",
    "for n in range(10):\n",
    "    r = random.randint(0,len(crime12))\n",
    "    x = crime12['State Abbreviation'].iloc[r] # Pull State Abreviation\n",
    "    y = crime12['GU Name'].iloc[r]  # Pull analogous county name\n",
    "    z = x+', '+y # Format name for dictionary key\n",
    "    agg = search(y,x) # Return criminal and economic data\n",
    "    samp_db[z] = agg  # Add data to empty dictionary\n",
    "print(samp_db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Arrest Options\n",
    "collect[0].columns[:48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function to clean the crime data and store it into a 'crime dictionary'. Will work if you update the 'crimes' variable at the top with new crimes to pull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Plotting & Collecting Specific Data\n",
    "crimes = 'DRGPOSS','AGASSLT','DUI'\n",
    "def searchcrime_cleaner(data,crimes,tsearch):\n",
    "    container = []\n",
    "    count = 0\n",
    "    for i in range(len(data)):\n",
    "        knight = data[count][tsearch].values\n",
    "        container.append(knight)\n",
    "        count += 1\n",
    "    container = container[0][0],container[1][0],container[2][0],container[3][0]\n",
    "    return container\n",
    "# Pull cleaned crime data, store in dictionary\n",
    "crime_dic = {}\n",
    "for i in crimes:\n",
    "    tsearch = i\n",
    "    crime_dic[i] = searchcrime_cleaner(collect[:4],crimes,tsearch)\n",
    "\n",
    "name = collect[0]['GU Name'].values, collect[0]['State Abbreviation'].values\n",
    "name = name[0]+\" County, \"+name[1]\n",
    "years = (2010,2012,2014,2016)\n",
    "#Plotting\n",
    "for i in crime_dic:\n",
    "    plt.plot(years,crime_dic[i],label=i)\n",
    "    plt.suptitle(\"Plot of Crime\",size=16)\n",
    "    plt.title(name[0], size=12)\n",
    "    plt.xlabel(\"Years\")\n",
    "    plt.ylabel(\"Number of Arrests\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "plt.savefig('presi_crimes')\n",
    "houdata = zillow2019.loc[(zillow2019['RegionName'].values == CountCit) & (zillow2019['State'].values == state)]\n",
    "try:\n",
    "    print(\"Average house price, 2019:  $\",int(houdata['Zhvi']))\n",
    "    print(\"Metro areas:\",houdata['Metro'].values[0])\n",
    "except:\n",
    "    print('--No Housing Data Available For This County--')\n",
    "print(crime_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Graphics for Industry and economic Data\n",
    "collect[0].columns[48:]    # Statistic options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I wrote a function to clean and store the industry data into a dictionary as well as a function to pull the 4 largest industries in our county. Although this is only ran with the individual county data (as opposed to finding the largest industries across the sample database, it shouldn't matter much since the most popular industries do not vary much from county to county)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for cleaning data from search variable\n",
    "def search_industrydata_cleaner(data,stat,tsearch):\n",
    "    container = []\n",
    "    count = 0\n",
    "    for n in range(len(data)):    # Loop for pulling data of multiple years      \n",
    "        container.append(data[n].loc[(data[n]['Industry']==tsearch)][stat].values[0])\n",
    "#                          dictionary//year       dictionary//year//column    comparison/DATA TO PULL\n",
    "        count += 1\n",
    "    #container = container[0][0],container[1][0],container[2][0],container[3][0],container[4][0]\n",
    "    return container # Return a list of individual values\n",
    "\n",
    "# Function to find largest industries per county\n",
    "def most_establishments(data):\n",
    "    data = data.loc[(data['Industry'] != '10 Total, all industries')] # Remove Totals, 2018\n",
    "    data = data.loc[(data['Industry'] != 'Total, all industries')] # Remove Totals, all other years\n",
    "    data = data.nlargest(4,'Establishment Count')\n",
    "    return data\n",
    "\n",
    "industry = most_establishments(collect[3])  # Returns 4 largest industries in 2016 ,index = 3\n",
    "industrydata = industry # Data set for other metrics in largest industries\n",
    "industry = list(industry['Industry']) # List of largest industries for reference purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stat = 'Establishment Count'  \n",
    "# Statistic associated with industry:: Establishment Count,Average Weekly Wage,September Employment\n",
    "indus_dic = {}\n",
    "for i in industry:\n",
    "    tsearch = i\n",
    "    rook = search_industrydata_cleaner(collect,stat,tsearch)\n",
    "    indus_dic[i] = list(rook)\n",
    "# Plotting Searched Data\n",
    "name = collect[0]['GU Name'].values,collect[0]['State Abbreviation'].values\n",
    "name = name[0][0]+\" County, \"+name[1][0]\n",
    "years = 2010,2012,2014,2016\n",
    "\n",
    "for i in indus_dic:\n",
    "    plt.plot(years,indus_dic[i],label=i)\n",
    "    plt.suptitle('Number of Establishments per 4 Biggest Industries')\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('name')\n",
    "indus_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Employment numbers\n",
    "stat = 'September Employment'  \n",
    "# Statistic associated with industry:: Establishment Count,Average Weekly Wage,September Employment\n",
    "emp_dic = {}\n",
    "for i in industry:\n",
    "    tsearch = i\n",
    "    rook = search_industrydata_cleaner(collect,stat,tsearch)\n",
    "    emp_dic[i] = rook\n",
    "# Plot\n",
    "for i in industry:\n",
    "    plt.plot(years,emp_dic[i],label=i)\n",
    "    plt.suptitle('Number of Jobs in Largest Industries')\n",
    "    plt.ylabel('Number of People Employed')\n",
    "    plt.xlabel('Year')\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('Industries with most employed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call Wage Numbers\n",
    "stat = 'Average Weekly Wage'  \n",
    "# Statistic associated with industry:: Establishment Count,Average Weekly Wage,September Employment\n",
    "wage_dic = {}\n",
    "for i in industry:\n",
    "    tsearch = i\n",
    "    rook = search_industrydata_cleaner(collect,stat,tsearch)\n",
    "    wage_dic[i] = rook\n",
    "# Plot\n",
    "for i in industry:\n",
    "    plt.plot(years,wage_dic[i],label=i)\n",
    "    plt.suptitle('Average Weekly Wages per Industry')\n",
    "    plt.ylabel('Average Weekly Wages')\n",
    "    plt.xlabel('Year')\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('Wages')\n",
    "print(wage_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REGRESSIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I figured out how to [sort of] do multiple regression for a single county. It runs, however the numbers are junk, as indicated by the near perfect R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I wrote a couple functions specifically designed to clean and format data for multiple counties to be easily stored inside a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function for cleaning data sample database\n",
    "def industrydata_cleaner(data,stat,tsearch):\n",
    "    container = []\n",
    "    for i in data:\n",
    "        for n in range(len(data[i])):\n",
    "            x = data[i][n].loc[data[i][n]['Industry'] == tsearch]\n",
    "            x = x[stat].values #[0]\n",
    "            container.append(x)\n",
    "    return container # Return a list of individual values\n",
    "\n",
    "def sample_crime_cleaner(data,crime):\n",
    "    container = []\n",
    "    for i in data:\n",
    "        for n in range(len(data[i])):\n",
    "            y = data[i][n][crime]\n",
    "            y = y.values[0]\n",
    "            container.append(y)\n",
    "    return container\n",
    "\n",
    "samp_dic = {}\n",
    "stat = 'Establishment Count','Average Weekly Wage','September Employment'\n",
    "for s in stat:\n",
    "    for n in industry:\n",
    "        samp_dic[n+' '+s] = industrydata_cleaner(samp_db,s,n)\n",
    "        \n",
    "crimes = collect[0].columns[:48] # Reset crimes to grab all crime data\n",
    "for i in crimes:\n",
    "    samp_dic[i] = sample_crime_cleaner(samp_db,i)\n",
    "finaldb = pd.DataFrame(data=samp_dic)\n",
    "# Reorganize columns\n",
    "#finaldb = finaldb[['Year','State Abbreviation','GU Name','Service-providing Establishment Count','Goods-producing Establishment Count','Trade, transportation, and utilities Establishment Count','Other services Establishment Count','Service-providing Average Weekly Wage','Goods-producing Average Weekly Wage','Trade, transportation, and utilities Average Weekly Wage','Other services Average Weekly Wage','Service-providing September Employment','Goods-producing September Employment','Trade, transportation, and utilities September Employment','Other services September Employment','MURDER','P1TOT','P1VLNT','P1PRPTY','RAPE','ROBBERY','AGASSLT','BURGLRY','LARCENY','MVTHEFT','ARSON','OTHASLT','FRGYCNT','FRAUD','EMBEZL','STLNPRP','VANDLSM','WEAPONS','COMVICE','SEXOFF','DRUGTOT','DRGSALE','COCSALE','MJSALE','SYNSALE','OTHSALE','DRGPOSS','COCPOSS','MJPOSS','SYNPOSS','OTHPOSS','GAMBLE','BOOKMKG','NUMBERS','OTGAMBL','OFAGFAM','DUI','LIQUOR','DRUNK','DISORDR','VAGRANT','ALLOTHR','SUSPICN','CURFEW','RUNAWAY']]\n",
    "#finaldb = finaldb[[ 'Year','State Abbreviation','GU Name','Service-providing Establishment Count','Education and health services Establishment Count','Professional and business services Establishment Count','Trade, transportation, and utilities Establishment Count','Service-providing Average Weekly Wage','Education and health services Average Weekly Wage','Professional and business services Average Weekly Wage','Trade, transportation, and utilities Average Weekly Wage','Service-providing September Employment','Education and health services September Employment','Professional and business services September Employment','Trade, transportation, and utilities September Employment','MURDER','P1TOT','P1VLNT','P1PRPTY','RAPE','ROBBERY','AGASSLT','BURGLRY','LARCENY','MVTHEFT','ARSON','OTHASLT','FRGYCNT','FRAUD','EMBEZL','STLNPRP','VANDLSM','WEAPONS','COMVICE','SEXOFF','DRUGTOT','DRGSALE','COCSALE','MJSALE','SYNSALE','OTHSALE','DRGPOSS','COCPOSS','MJPOSS','SYNPOSS','OTHPOSS','GAMBLE','BOOKMKG','NUMBERS','OTGAMBL','OFAGFAM','DUI','LIQUOR','DRUNK','DISORDR',`'VAGRANT','ALLOTHR','SUSPICN','CURFEW','RUNAWAY']]\n",
    "print(\"Counties in Sample Data: \",samp_db.keys(),'\\n')\n",
    "#print(\"Columns in final database: \",finaldb.columns)\n",
    "print('\\nFinal Database for all 10 counties over 4 years in my sample.')\n",
    "finaldb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a regression of the number of establishments in each of the largest industries on drug possession arrests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = finaldb[list(finaldb.columns.values[3:7])]  # Use only the first 3 columns to regress\n",
    "y = finaldb['DRGPOSS']  # Set y variable\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a visualization of the part of the dataframe I ran a regression on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(finaldb['Service-providing Estab. Count'],y,color='b',label='Service-providing Estab. Count',alpha=0.5)\n",
    "plt.scatter(finaldb['Trade, transportation, and utilities Estab. Count'],y,color='y',alpha=0.5,label='Trade, transportation, and utilities Estab. Count')\n",
    "plt.scatter(finaldb['Professional and business services Estab. Count'],y,color='r',alpha=0.5,label='Professional and business services Estab. Count')\n",
    "plt.scatter(finaldb['Education and health services Estab. Count'],y,color='g',alpha=0.5,label='Education and health services Estab. Count')\n",
    "\n",
    "plt.ylabel('Number of DRGPOSS arrests')\n",
    "plt.xlabel('Number of Establishments')\n",
    "plt.title('Number of Establishments per Industry over \\n Number of Drug Possession Arrests')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "#plt.savefig('Finalreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see some pretty serious outliers on my graph. So I'll try to control for population. \n",
    "\n",
    "The cell immedietly below this text will pull the largest cities based on total property crimes greater than 15,000. As it turns out this is a poor control for population and we still get some significant outliers so I end up dropping those to. We are left with 11 major counties; 4 in CA, 4 in NY, 2 in TX, Philadelphia, Las Vegas, and Maricopa AZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of counties with PITOTs of greater than 10,000\n",
    "masked_Lc10 = db2010.mask(db2010['P1TOT'] < 15000)\n",
    "masked_Lc10 = masked_Lc10.dropna(axis = 0, how='any')\n",
    "masked_Lc10 = masked_Lc10.reset_index()  # resets the index to easily pull random counties by index\n",
    "#print(masked_Lc10['DRGPOSS']) # Used to see what outliers to drop\n",
    "masked_Lc10 = masked_Lc10.drop(masked_Lc10.index[[1,5,12,14]]) # Drop outliers by index number\n",
    "\n",
    "Largest_list = list(masked_Lc10['GU Name']+', '+masked_Lc10['State Abbreviation'])\n",
    "Largest_counties = list(masked_Lc10['GU Name'])\n",
    "Largest_states = list(masked_Lc10['State Abbreviation'])\n",
    "\n",
    "control_db = {}\n",
    "count = 0\n",
    "for i in Largest_states:\n",
    "    control_db[Largest_list[count]] = search(Largest_counties[count],i)\n",
    "    count += 1\n",
    "\n",
    "control_db.keys() # Returns 15 largest counties by P1TOT approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to analyze the largest cities, skip this cell.**\n",
    "\n",
    "Here is the cell for smallest/medium sized counties. You can see where I set the upper and lower bounds of P1TOTs (total property crimes as a rough control for population) I want to pull. The loop folllowing will randomly pull 50 counties from the dataframe in preperation for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for counties with 1,000 - 5,000 P1TOTs to control for population\n",
    "masked_Sc10 = db2010.mask(db2010['P1TOT'] > 6000) # Mask counties with greater than 5000 P1TOTs\n",
    "masked_Sc10 = masked_Sc10.mask(masked_Sc10['P1TOT'] < 4500)\n",
    "masked_Sc10 = masked_Sc10.dropna(axis = 0, how='any')\n",
    "masked_Sc10 = masked_Sc10.reset_index()  # resets the index to easily pull random counties by index\n",
    "\n",
    "control_db = {}\n",
    "count = 0\n",
    "# Will pull x counties, same as largest counties returns\n",
    "for n in range(50):\n",
    "    r = random.randint(0,len(masked_Sc10)-1)\n",
    "    x = masked_Sc10['State Abbreviation'].iloc[r] # Pull State Abreviation\n",
    "    y = masked_Sc10['GU Name'].iloc[r]  # Pull analogous county name\n",
    "    z = x+', '+y # Format name for dictionary key\n",
    "    bop = search(y,x) # Return criminal and economic data\n",
    "    control_db[z] = bop  # Add data to empty dictionary\n",
    "\n",
    "print('Counties in the sample:',control_db.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will clean whatever population controlled sample you chose and store it into a neatly formatted dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dic = {}\n",
    "stat = 'Establishment Count','Average Weekly Wage','September Employment'\n",
    "for s in stat:\n",
    "    for n in industry:\n",
    "        pop_dic[n+' '+s] = industrydata_cleaner(control_db,s,n)\n",
    "\n",
    "for i in crimes:\n",
    "    pop_dic[i] = sample_crime_cleaner(control_db,i)\n",
    "popdb = pd.DataFrame(data=pop_dic)\n",
    "popdb = popdb[[ 'Year','State Abbreviation','GU Name','Service-providing Establishment Count','Education and health services Establishment Count','Professional and business services Establishment Count','Trade, transportation, and utilities Establishment Count','Service-providing Average Weekly Wage','Education and health services Average Weekly Wage','Professional and business services Average Weekly Wage','Trade, transportation, and utilities Average Weekly Wage','Service-providing September Employment','Education and health services September Employment','Professional and business services September Employment','Trade, transportation, and utilities September Employment','MURDER','P1TOT','P1VLNT','P1PRPTY','RAPE','ROBBERY','AGASSLT','BURGLRY','LARCENY','MVTHEFT','ARSON','OTHASLT','FRGYCNT','FRAUD','EMBEZL','STLNPRP','VANDLSM','WEAPONS','COMVICE','SEXOFF','DRUGTOT','DRGSALE','COCSALE','MJSALE','SYNSALE','OTHSALE','DRGPOSS','COCPOSS','MJPOSS','SYNPOSS','OTHPOSS','GAMBLE','BOOKMKG','NUMBERS','OTGAMBL','OFAGFAM','DUI','LIQUOR','DRUNK','DISORDR','VAGRANT','ALLOTHR','SUSPICN','CURFEW','RUNAWAY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "popdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variation among small and medium sized counties is very large. I have gotten some seemingly great looking regressions with statistically significant P-values and realistic R-squared -- and I have gotten some near perfect R-squareds with very insignificant P-values.\n",
    "\n",
    "I have also made it very easy to switch between which variables to regress on. You will notice the first 3 columns of my dataframe are the year, state, and county so we can leave those out. columns 4-15 are the number of establishments, average weekly wages, and number of people employed in the 4 largest industries. The remaining rows are all crime variables. \n",
    "\n",
    "Running the regression on all economic variables doesn't turn out much better than just regressing on 'number of establishments'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = popdb[list(popdb.columns.values[4:15])]  # Use only the first 3 columns to regress\n",
    "y = popdb['DRGPOSS']  # Set y variable\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am simply making some plots to show the wage and establishment data in the sample plotted against whatever crime you regressed against (changing the crime will make the graph labels inaccurate but the data will still plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2,sharey=False,sharex=False) # The graphs could share a y axis but I feel it is more readable with the numbers plotted on both axis\n",
    "fig.set_figheight(7)\n",
    "fig.set_figwidth(10)\n",
    "ax1.scatter(popdb['Service-providing Estab. Count'],y,color='b',label='Service-providing Estab. Count',alpha=0.5)\n",
    "ax1.scatter(popdb['Trade, transportation, and utilities Estab. Count'],y,color='y',alpha=0.5,label='Trade, transportation, and utilities Estab. Count')\n",
    "ax1.scatter(popdb['Professional and business services Estab. Count'],y,color='r',alpha=0.5,label='Professional and business services Estab. Count')\n",
    "ax1.scatter(popdb['Education and health services Estab. Count'],y,color='g',alpha=0.5,label='Education and health services Wages')\n",
    "\n",
    "ax2.scatter(popdb['Service-providing Avg. Wkly Wage'],y,color='b',label='Service-providing Wages',alpha=0.5)\n",
    "ax2.scatter(popdb['Trade, transportation, and utilities Avg. Wkly Wage'],y,color='y',alpha=0.5,label='Trade, transportation, and utilities Wages')\n",
    "ax2.scatter(popdb['Professional and business services Avg. Wkly Wage'],y,color='r',alpha=0.5,label='Professional and business services Wages')\n",
    "ax2.scatter(popdb['Education and health services Avg. Wkly Wage'],y,color='g',alpha=0.5,label='Education and health services Wages')\n",
    "\n",
    "\n",
    "ax1.set_ylabel('Number of DRGPOSS arrests')\n",
    "ax1.set_xlabel('Number of Establishments')\n",
    "ax2.set_xlabel('Average Weekly Wages')\n",
    "ax1.set_title('Number of Establishments per Industry over \\n Number of Drug Possession Arrests')\n",
    "ax2.set_title('Average Weekly Wage per Industry over \\n Number of Drug Possession Arrests')\n",
    "ax2.legend(loc='best')\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "plt.savefig('Finalreg')\n",
    "#print(\"Cities with Largest/Smallest P1TOTs:\",list(control_db.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a visualization of the samples wage and establishment data plotted together with a double y axis for scaleing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.scatter(y,popdb['Service-providing Estab. Count'],color='b',label='Service-providing',alpha=0.5)\n",
    "ax1.scatter(y,popdb['Trade, transportation, and utilities Estab. Count'],color='y',alpha=0.5,label='Trade, transportation, and utilities')\n",
    "ax1.scatter(y,popdb['Professional and business services Estab. Count'],color='r',alpha=0.5,label='Professional and business services')\n",
    "ax1.scatter(y,popdb['Education and health services Estab. Count'],color='g',alpha=0.5,label='Education and health services')\n",
    "ax1.set_ylabel('Establishments per Industry')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.scatter(y,popdb['Service-providing Avg. Wkly Wage'],color='b',label='Service-providing',alpha=0.5)\n",
    "ax2.scatter(y,popdb['Trade, transportation, and utilities Avg. Wkly Wage'],color='y',alpha=0.5,label='Trade, transportation, and utilities')\n",
    "ax2.scatter(y,popdb['Professional and business services Avg. Wkly Wage'],color='r',alpha=0.5,label='Professional and business services')\n",
    "ax2.scatter(y,popdb['Education and health services Avg. Wkly Wage'],color='g',alpha=0.5,label='Education and health services')\n",
    "ax2.set_ylabel('Wages per Industry')\n",
    "ax2.set_xlabel('Drug Possession Arrests')\n",
    "plt.title('Wages & Establishment Counts \\n Plotted Against Drug Possession Arrests')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('Finalreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
