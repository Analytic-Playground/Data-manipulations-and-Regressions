{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<meta property='og:image' content='https://www.information-age.com/wp-content/uploads/2018/11/data-era-1013x440.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.information-age.com/wp-content/uploads/2018/11/data-era-1013x440.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Crime & Economy (in python) </center></h1>\n",
    "\n",
    "In this project I look for a relationship between economic data and crime rates. \n",
    "\n",
    "I obtained statistics for various industries at the county level from the Bureau of Labor Statistics. I also found the number of arrests and charges for a variety of crimes at the county level from the Inter-University Consortium for Political and Social Research. These datasets span from 2009, 2010, 2012, 2014, and 2016.\n",
    "\n",
    "After cleaning the data I will test out some statistical analysis.\n",
    "\n",
    "\\---------------------------------------------------------------------------\n",
    "\n",
    "BLS, link to data: https://www.bls.gov/cew/datatoc.htm\n",
    "\n",
    "ICPSR / Crime, link to data: https://www.icpsr.umich.edu/icpsrweb/NACJD/series/57 (seires name: \"Uniform Crime Reporting Program Data: County-Level Detailed Arrest and Offense Data, United States, yyyy\")\n",
    "\n",
    "Authors Linkedin: https://www.linkedin.com/in/matt-krieger-094/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project\n",
    "%run Imports.ipynb\n",
    "# Imports functions from seperate notebook\n",
    "%run Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I load in the data and spit out some summary statistics on the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Data\n",
    "start_time = timeit.default_timer()\n",
    "# Keys to extract formatted State and County names\n",
    "key00 = pd.read_csv('FIPS_countycodes.txt',delimiter=',')  # Counties only\n",
    "key00['GU Name'] = key00['GU Name'].str[:-7]  # Slice 'county' from names\n",
    "# Crime\n",
    "crime2009 = pd.read_csv('30763-0001-crimedata2009.txt',delimiter='\\t')\n",
    "crime2009['year'] = 2009\n",
    "crime2010 = pd.read_csv('33523-0001-crimedata2010.txt',delimiter='\\t')\n",
    "crime2010['year'] = 2010\n",
    "crime2012 = pd.read_csv('35019-0001-crimedata2012.txt',delimiter='\\t')\n",
    "crime2012['year'] = 2012\n",
    "crime2014 = pd.read_csv('36399-0001-crimedata2014.txt',delimiter='\\t')\n",
    "crime2014['year'] = 2014\n",
    "crime2016 = pd.read_csv('37059-0001-crimedata2016.txt',delimiter='\\t')\n",
    "crime2016['year'] = 2016\n",
    "# Enumployment & Wages\n",
    "wage_unemp2018 = pd.read_csv('allhlcn183wage_unemp2018.txt',delimiter='\\t',dtype={'St':str})\n",
    "wage_unemp2016 = pd.read_csv('allhlcn163wage_unemp2016.txt',delimiter='\\t',dtype={'St':str})\n",
    "wage_unemp2014 = pd.read_csv('allhlcn143wage_unemp2014.txt',delimiter='\\t',dtype={'St':str})\n",
    "wage_unemp2012 = pd.read_csv('allhlcn123wage_unemp2012.txt',delimiter='\\t',dtype={'St':str})\n",
    "wage_unemp2010 = pd.read_csv('allhlcn103wage_unemp2010.txt',delimiter='\\t',dtype={'St':str})\n",
    "wage_unemp2009 = pd.read_csv('allhlcn093wage_unemp2009.txt',delimiter='\\t',dtype={'St':str})\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(round(elapsed,3),'run time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge & clean all economic data\n",
    "start_time = timeit.default_timer()\n",
    "pd.options.mode.chained_assignment = None # Turn off copy with setting warning\n",
    "econ00 = pd.concat([wage_unemp2009,wage_unemp2010,wage_unemp2012,wage_unemp2014,wage_unemp2016],0)\n",
    "econ01 = wage_cleaner(econ00)\n",
    "# Get columns for bot to convert floats into ints\n",
    "z = econ01.columns.tolist()\n",
    "y = z[-8:]\n",
    "econ02 = bot97(econ01)\n",
    "# Merge & clean all criminal data\n",
    "crime00 = pd.concat([crime2009,crime2010,crime2012,crime2014,crime2016],0)\n",
    "crime01 = crime_cleaner(crime00)\n",
    "crime01 = crime01.drop([15681,15682,15683,15684,15685]) # drop counties only existing in one year (by index)\n",
    "# Merge economic & criminal data\n",
    "db = crime01.merge(econ02,left_on=['State Abbreviation','GU Name','year'],right_on=['State Abbreviation','GU Name','Year'])\n",
    "cols = db.columns.tolist()\n",
    "cols = cols[:3]+cols[49:]+cols[3:48]\n",
    "db = db[cols]\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(round(elapsed,3),'run time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I generate a sample dataset of randomly selected counties and all associated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "names = db.drop_duplicates(subset=['GU Name','State Abbreviation']) # Min df to pull names from\n",
    "samp_db = {}\n",
    "for a in range(200):\n",
    "    i = random.randint(0,len(names))\n",
    "    x = names['State Abbreviation'].iloc[i] # Pull State Abreviation\n",
    "    y = names['GU Name'].iloc[i]  # Pull analogous county name\n",
    "    z = x+', '+y # Format name for dictionary key\n",
    "    data = db.loc[(db['State Abbreviation'] == x) & (db['GU Name'] == y)]\n",
    "    samp_db[z] = data\n",
    "keys = list(samp_db.keys())\n",
    "print(len(samp_db.keys()))\n",
    "print(samp_db.keys())\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(round(elapsed,3),'run time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 biggest industries in sample\n",
    "industry = big_industry_bot(samp_db)\n",
    "print('5 Largest Industries in sample:\\n',industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start base data frame -- create new dataframe, still need to account for various industries\n",
    "start_time = timeit.default_timer()\n",
    "bedrock = samp_db # Initialize sample to new variable for trouble shooting // preserve OG sample data\n",
    "new_world = pd.DataFrame() # empty dataframe\n",
    "\n",
    "for a in keys:\n",
    "    for b in industry[:]:\n",
    "        x = bedrock[a].loc[bedrock[a]['Industry'] == b] # Pull\n",
    "        new_world = new_world.append(x)\n",
    "        \n",
    "new_world = new_world.reset_index()  # reset index\n",
    "new_world = new_world.drop('index',axis=1)  # drop index column\n",
    "cols = new_world.columns.tolist()\n",
    "cols = cols[:3]+cols[7:]\n",
    "new_world = new_world[cols]\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(round(elapsed,3),'run time')\n",
    "new_world.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to inconsistencies in the data it is important that we double check that there is the same number of data entries for each industry we want to observe. If we have an uneven database, we need to draw a new sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check counts of target data to ensure symmetry/integrity for analysis\n",
    "bot98(new_world,industry)  # Call function to check data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to account for changes in population size, we must convert all of our data into percentage changes from a base year (2009). This transformation will cause us to lose a year of data entries but will provide a more robust model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert dataframe into percent changes\n",
    "start_time = timeit.default_timer()\n",
    "z = new_world.columns.tolist()\n",
    "a = new_world[z[:4]] # Split names and catagories off of dataframe;; Quicker runtime\n",
    "b = new_world[z[4:]] # Split numbers into seperate dataframe for percentage changes\n",
    "tail = 0\n",
    "head = 5\n",
    "# .iloc using loop to apply percent change for every 5 years\n",
    "for i in range(int(len(b)/5)):\n",
    "    b.iloc[tail:head] = b.iloc[tail:head][:].pct_change()\n",
    "    head += 5                                \n",
    "    tail += 5\n",
    "new_world = pd.concat([a,b],1) # Recombind dataframes\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(round(elapsed,3),'run time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy Cnty&St -> split crimes off -> get rid of duplicates\n",
    "# Reapply crimes to reformatted economic dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create final data frame that accounts for metrics across industries\n",
    "start_time = timeit.default_timer()\n",
    "samp_dic = {}\n",
    "stat = ['Establishment Count','Average Weekly Wage','August Employment']\n",
    "# Pull and reformat target stats and industries\n",
    "for s in stat:\n",
    "    for n in industry:\n",
    "        samp_dic[n+' '+s] = test_func(new_world,keys,s,n)\n",
    "# Convert dictionary to data frame  \n",
    "finaldb = pd.DataFrame(data=samp_dic)\n",
    "# labels for rematching\n",
    "labels = new_world[['year','State Abbreviation','GU Name']].drop_duplicates(subset=['State Abbreviation','GU Name','year'])\n",
    "# Split off crime data\n",
    "crimedb = new_world.drop_duplicates(subset=['State Abbreviation','GU Name','year'])\n",
    "cols = crimedb.columns.tolist()\n",
    "crimedb = crimedb[cols[12:]] # Select only crime columns\n",
    "# Apply labels to crime and econ data\n",
    "crime = pd.concat([labels,crimedb],1)\n",
    "econ = pd.concat([labels,finaldb],1)\n",
    "finaldb = econ.merge(crime,left_on=['State Abbreviation','GU Name','year'],right_on=['State Abbreviation','GU Name','year'])\n",
    "finaldb['year'] = finaldb['year'].astype(int) # convert year from float to int\n",
    "print(samp_dic.keys(),'\\n\\n')\n",
    "# Remove NAs and infinite values\n",
    "cols = finaldb.columns.tolist()\n",
    "finaldb[cols[18:]] = finaldb[cols[18:]].fillna(0) # Fill all NaNs with 0's in crime data(because cannot divide by zero)\n",
    "finaldb = finaldb.dropna(how='any').replace(-np.inf,-0.99).replace(np.inf,0.99)  # drop all 2010/base year data\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(round(elapsed,3),'run time')\n",
    "finaldb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrdb = finaldb.corr().abs().sum()\n",
    "corrdbl = pd.DataFrame(data=corrdb).nlargest(11,0).transpose()  # Largest correlations\n",
    "corrdbs = pd.DataFrame(data=corrdb).nsmallest(10,0).transpose()   # Smallest correlations\n",
    "# Creates a correlation matrix of all variables\n",
    "plt.matshow(finaldb.corr())\n",
    "plt.suptitle('Correlation Matrix Of All Variables\\n')\n",
    "plt.show()\n",
    "cols = finaldb.columns.tolist()\n",
    "print('\\n11 HIGHEST total correlations:')\n",
    "corrdbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = corrdbl.columns.tolist() # variables to regress on\n",
    "exp = l[1:] # Selects variable with highest total correlation to use as dependant variable\n",
    "dep = l[0] # Uses next 10 most correlated variables to act as independant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION\n",
    "\n",
    "Regression is of most correlated variable on next 10 most correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = finaldb[exp]  # Select columns to use as independant vars\n",
    "y = finaldb[dep]  # Set y variable\n",
    "\n",
    "model = sm.OLS(y, x.astype(float)).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression of most correlated variable on all economic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = finaldb[finaldb.columns.tolist()[3:18]]  # Selects all economic variables\n",
    "y = finaldb[dep]  # Set y variable\n",
    "\n",
    "model = sm.OLS(y, x.astype(float)).fit()\n",
    "predictions = model.predict(x) # make the predictions by the model\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Summary\n",
    "\n",
    "The first regression likely has the highest R-squared in our sample. Perhaps unsurprisingly we see that crime rates are more related to each other than with or between economic data. We also see that aggregate variables tend to be the strongest which makes sense again due to their interconnected nature.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the second regression we can see the most correlated crime rate (total part 1 crimes; an aggregate of murder, rape, robbery, aggravated assault, burglary, larceny, auto theft, and arson) regressed on all of our economic variables. It is a substantially lower R-squared value and is no longer significant. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
